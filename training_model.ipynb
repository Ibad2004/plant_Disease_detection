{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6163d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a38d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef723561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REMOVE CORRUPTED IMAGES\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "RAW_DATASET_DIR = 'raw_dataset'  # Your original dataset folder\n",
    "\n",
    "def remove_corrupted_images(dataset_dir):\n",
    "    removed_files = []\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()  # Verify if image is corrupted\n",
    "            except (IOError, SyntaxError) as e:\n",
    "                print(f\"Removing corrupted image: {img_path}\")\n",
    "                os.remove(img_path)\n",
    "                removed_files.append(img_path)\n",
    "    return removed_files\n",
    "\n",
    "removed = remove_corrupted_images(RAW_DATASET_DIR)\n",
    "print(f\"Total corrupted images removed: {len(removed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038282a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CLEAN LABELS\n",
    "import re\n",
    "\n",
    "def check_class_names(dataset_dir):\n",
    "    invalid_names = []\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        # Allow only alphanumeric, underscore, and spaces\n",
    "        if not re.match(r'^[\\w\\s]+$', class_name):\n",
    "            invalid_names.append(class_name)\n",
    "    return invalid_names\n",
    "\n",
    "invalid_classes = check_class_names(RAW_DATASET_DIR)\n",
    "if invalid_classes:\n",
    "    print(\"Invalid class folder names detected:\")\n",
    "    for name in invalid_classes:\n",
    "        print(f\" - {name}\")\n",
    "else:\n",
    "    print(\"All class folder names are clean!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing Images and Converting to JPEG\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def resize_and_convert(dataset_dir, output_dir, target_size):\n",
    "    total_images = 0\n",
    "    total_skipped = 0\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        processed_class_path = os.path.join(output_dir, class_name.replace(' ', '_'))\n",
    "        os.makedirs(processed_class_path, exist_ok=True)\n",
    "\n",
    "        count = 0\n",
    "        for img_name in os.listdir(class_path):\n",
    "            src_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                with Image.open(src_path) as img:\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "                    base_name = os.path.splitext(img_name)[0]\n",
    "                    save_path = os.path.join(processed_class_path, base_name + '.jpg')\n",
    "                    img.save(save_path, 'JPEG', quality=95)\n",
    "                count += 1\n",
    "                total_images += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid image {src_path}: {e}\")\n",
    "                total_skipped += 1\n",
    "        print(f\"Processed {count} images in class '{class_name}'\")\n",
    "\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "    print(f\"Total images skipped: {total_skipped}\")\n",
    "\n",
    "# Example usage:\n",
    "resize_and_convert('raw_dataset', 'processed_dataset', (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4735ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_images(dataset_dir):\n",
    "    total_images = 0\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"Class '{class_name}': {num_images} images\")\n",
    "        total_images += num_images\n",
    "    print(f\"Total images in dataset: {total_images}\")\n",
    "    return total_images\n",
    "\n",
    "count_images('processed_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82db2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_images(dataset_dir):\n",
    "    total_images = 0\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        num_images = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"Class '{class_name}': {num_images} images\")\n",
    "        total_images += num_images\n",
    "    print(f\"Total images in dataset: {total_images}\")\n",
    "    return total_images\n",
    "\n",
    "count_images('raw_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "PROCESSED_DATASET_DIR = 'processed_dataset'  # update path if different\n",
    "\n",
    "def dhash(image, hash_size=8):\n",
    "    # Use LANCZOS resampling instead of deprecated ANTIALIAS\n",
    "    image = image.convert('L').resize((hash_size + 1, hash_size), Image.Resampling.LANCZOS)\n",
    "    pixels = list(image.getdata())\n",
    "    difference = []\n",
    "    for row in range(hash_size):\n",
    "        for col in range(hash_size):\n",
    "            left_pixel = pixels[row * (hash_size + 1) + col]\n",
    "            right_pixel = pixels[row * (hash_size + 1) + col + 1]\n",
    "            difference.append(left_pixel > right_pixel)\n",
    "    decimal_value = 0\n",
    "    hex_string = []\n",
    "    for index, value in enumerate(difference):\n",
    "        if value:\n",
    "            decimal_value += 2 ** (index % 8)\n",
    "        if (index % 8) == 7:\n",
    "            hex_string.append(hex(decimal_value)[2:].rjust(2, '0'))\n",
    "            decimal_value = 0\n",
    "    return ''.join(hex_string)\n",
    "\n",
    "def remove_duplicates(dataset_dir):\n",
    "    hashes = set()\n",
    "    removed_count = 0\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    h = dhash(img)\n",
    "                if h in hashes:\n",
    "                    print(f\"Duplicate found, removing: {img_path}\")\n",
    "                    os.remove(img_path)\n",
    "                    removed_count += 1\n",
    "                else:\n",
    "                    hashes.add(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "    return removed_count\n",
    "\n",
    "duplicates_removed = remove_duplicates(PROCESSED_DATASET_DIR)\n",
    "print(f\"Duplicates removed: {duplicates_removed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa18cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "# Set the processed dataset directory\n",
    "DATASET_DIR = 'processed_dataset'\n",
    "\n",
    "# ImageDataGenerator with augmentation settings\n",
    "augmentor = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# Initialize total counters\n",
    "total_original_images = 0\n",
    "total_augmented_images = 0\n",
    "\n",
    "# Process each class\n",
    "for class_name in os.listdir(DATASET_DIR):\n",
    "    class_path = os.path.join(DATASET_DIR, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📁 Processing class: {class_name}\")\n",
    "    image_count = 0\n",
    "    augmented_count = 0\n",
    "\n",
    "    for img_name in os.listdir(class_path):\n",
    "        if img_name.startswith('aug_'):\n",
    "            continue  # Skip already augmented images\n",
    "\n",
    "        try:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = load_img(img_path)\n",
    "            x = img_to_array(img)\n",
    "            x = x.reshape((1,) + x.shape)\n",
    "\n",
    "            # Generate 3 augmented images\n",
    "            i = 0\n",
    "            for batch in augmentor.flow(\n",
    "                x,\n",
    "                batch_size=1,\n",
    "                save_to_dir=class_path,\n",
    "                save_prefix='aug',\n",
    "                save_format='jpeg'\n",
    "            ):\n",
    "                i += 1\n",
    "                augmented_count += 1\n",
    "                total_augmented_images += 1\n",
    "                if i >= 3:\n",
    "                    break\n",
    "\n",
    "            image_count += 1\n",
    "            total_original_images += 1\n",
    "            print(f\"✅ Augmented 3 images for: {img_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error augmenting image {img_name}: {e}\")\n",
    "\n",
    "    print(f\"📊 Class summary — Original: {image_count}, Augmented: {augmented_count}\")\n",
    "\n",
    "print(\"\\n✅ Augmentation complete\")\n",
    "print(f\"🔢 Total original images processed: {total_original_images}\")\n",
    "print(f\"📈 Total augmented images created: {total_augmented_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc31bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define your augmentation pipeline\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
    "])\n",
    "\n",
    "def find_minority_classes(dataset_dir, threshold=3500):\n",
    "    minority_classes = []\n",
    "    print(\"🔍 Checking classes for minority status based on threshold ≤\", threshold)\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        num_images = len([name for name in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, name))])\n",
    "        print(f\"Class '{class_name}' has {num_images} images\")\n",
    "        if num_images <= threshold:\n",
    "            minority_classes.append(class_name)\n",
    "            print(f\"✅ Class '{class_name}' marked as minority class\")\n",
    "    return minority_classes\n",
    "\n",
    "def augment_minority_classes(dataset_dir, minority_classes, augmentations_per_image=3):\n",
    "    total_original = 0\n",
    "    total_augmented = 0\n",
    "\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        if class_name not in minority_classes:\n",
    "            print(f\"⏭ Skipping majority class: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n🚀 Augmenting minority class: {class_name}\")\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            if img_name.startswith('aug_') or '_aug' in img_name:\n",
    "                # Skip already augmented images to avoid duplication\n",
    "                continue\n",
    "            \n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    total_original += 1\n",
    "                    base_name, ext = os.path.splitext(img_name)\n",
    "                    \n",
    "                    for i in range(augmentations_per_image):\n",
    "                        augmented_img = augmentation(img)\n",
    "                        augmented_name = f\"{base_name}_aug{i+1}.jpg\"\n",
    "                        augmented_path = os.path.join(class_path, augmented_name)\n",
    "                        augmented_img.save(augmented_path, 'JPEG', quality=95)\n",
    "                        total_augmented += 1\n",
    "\n",
    "                    print(f\"✅ Augmented {augmentations_per_image} images for: {img_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error augmenting {img_path}: {e}\")\n",
    "\n",
    "    print(\"\\n✅ Augmentation complete\")\n",
    "    print(f\"🔢 Total original images processed for minority classes: {total_original}\")\n",
    "    print(f\"📈 Total augmented images created for minority classes: {total_augmented}\")\n",
    "\n",
    "# Set your processed dataset directory path here\n",
    "PROCESSED_DATASET_DIR = 'processed_dataset'\n",
    "\n",
    "# Step 1: Find minority classes automatically\n",
    "minority_classes = find_minority_classes(PROCESSED_DATASET_DIR, threshold=3500)\n",
    "print(\"\\n🔎 Minority classes detected:\", minority_classes)\n",
    "\n",
    "# Step 2: Augment only minority classes\n",
    "augment_minority_classes(PROCESSED_DATASET_DIR, minority_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define your augmentation pipeline\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n",
    "])\n",
    "\n",
    "def find_minority_classes(dataset_dir, threshold=2000):\n",
    "    minority_classes = []\n",
    "    print(\"🔍 Checking classes for minority status based on threshold ≤\", threshold)\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        num_images = len([name for name in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, name))])\n",
    "        print(f\"Class '{class_name}' has {num_images} images\")\n",
    "        if num_images <= threshold:\n",
    "            minority_classes.append(class_name)\n",
    "            print(f\"✅ Class '{class_name}' marked as minority class\")\n",
    "    return minority_classes\n",
    "\n",
    "def augment_minority_classes(dataset_dir, minority_classes, augmentations_per_image=3):\n",
    "    total_original = 0\n",
    "    total_augmented = 0\n",
    "\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        if class_name not in minority_classes:\n",
    "            print(f\"⏭ Skipping majority class: {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n🚀 Augmenting minority class: {class_name}\")\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    total_original += 1\n",
    "                    base_name, ext = os.path.splitext(img_name)\n",
    "                    \n",
    "                    for i in range(augmentations_per_image):\n",
    "                        augmented_img = augmentation(img)\n",
    "                        augmented_name = f\"{base_name}_aug{i+1}.jpg\"\n",
    "                        augmented_path = os.path.join(class_path, augmented_name)\n",
    "                        augmented_img.save(augmented_path, 'JPEG', quality=95)\n",
    "                        total_augmented += 1\n",
    "\n",
    "                    print(f\"✅ Augmented {augmentations_per_image} images for: {img_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error augmenting {img_path}: {e}\")\n",
    "\n",
    "    print(\"\\n✅ Augmentation complete\")\n",
    "    print(f\"🔢 Total images processed for minority classes: {total_original}\")\n",
    "    print(f\"📈 Total augmented images created for minority classes: {total_augmented}\")\n",
    "\n",
    "# Set your processed dataset directory path here\n",
    "PROCESSED_DATASET_DIR = 'processed_dataset'\n",
    "\n",
    "# Step 1: Find minority classes automatically\n",
    "minority_classes = find_minority_classes(PROCESSED_DATASET_DIR, threshold=2000)\n",
    "print(\"\\n🔎 Minority classes detected:\", minority_classes)\n",
    "\n",
    "# Step 2: Augment only minority classes\n",
    "augment_minority_classes(PROCESSED_DATASET_DIR, minority_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8905146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ======================\n",
    "# 🔧 CONFIGURATION\n",
    "# ======================\n",
    "DATASET_DIR = 'processed_dataset'  # Folder where class subfolders are\n",
    "LABEL_MAPPING_FILE = 'label_mapping.json'\n",
    "\n",
    "def encode_labels(dataset_dir, save_mapping_file):\n",
    "    print(\"🔍 Scanning dataset for class labels...\")\n",
    "\n",
    "    # Get list of folder names (each folder = one class label)\n",
    "    class_labels = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "    print(f\"🗂 Found {len(class_labels)} classes:\")\n",
    "    for label in class_labels:\n",
    "        print(f\"  - {label}\")\n",
    "\n",
    "    # Sort labels and assign integers\n",
    "    label_to_int = {label: idx for idx, label in enumerate(sorted(class_labels))}\n",
    "    print(\"\\n🔢 Label to integer mapping:\")\n",
    "    for label, idx in label_to_int.items():\n",
    "        print(f\"  '{label}' -> {idx}\")\n",
    "\n",
    "    # Save mapping to JSON for future reference\n",
    "    with open(save_mapping_file, 'w') as f:\n",
    "        json.dump(label_to_int, f, indent=4)\n",
    "    print(f\"\\n💾 Saved label mapping to '{save_mapping_file}'\")\n",
    "\n",
    "    print(\"\\n✅ Label encoding done!\")\n",
    "    return label_to_int\n",
    "\n",
    "# Run encoding\n",
    "label_mapping = encode_labels(DATASET_DIR, LABEL_MAPPING_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# ======================\n",
    "# 🔧 CONFIGURATION\n",
    "# ======================\n",
    "DATASET_DIR = 'processed_dataset'  # Original dataset folder with class subfolders\n",
    "OUTPUT_DIR = 'dataset_split'       # Folder to create train/, val/, and test/ folders inside\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.2\n",
    "SEED = 42  # For reproducibility\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def split_dataset(dataset_dir, output_dir, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "\n",
    "    train_dir = os.path.join(output_dir, 'train')\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "    test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        train_class_dir = os.path.join(train_dir, class_name)\n",
    "        val_class_dir = os.path.join(val_dir, class_name)\n",
    "        test_class_dir = os.path.join(test_dir, class_name)\n",
    "\n",
    "        os.makedirs(train_class_dir, exist_ok=True)\n",
    "        os.makedirs(val_class_dir, exist_ok=True)\n",
    "        os.makedirs(test_class_dir, exist_ok=True)\n",
    "\n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "\n",
    "        train_images = images[:n_train]\n",
    "        val_images = images[n_train:n_train + n_val]\n",
    "        test_images = images[n_train + n_val:]\n",
    "\n",
    "        for img_name in train_images:\n",
    "            shutil.copy2(os.path.join(class_path, img_name), os.path.join(train_class_dir, img_name))\n",
    "\n",
    "        for img_name in val_images:\n",
    "            shutil.copy2(os.path.join(class_path, img_name), os.path.join(val_class_dir, img_name))\n",
    "\n",
    "        for img_name in test_images:\n",
    "            shutil.copy2(os.path.join(class_path, img_name), os.path.join(test_class_dir, img_name))\n",
    "\n",
    "    print(f\"✅ Dataset split completed with {train_ratio*100}% train, {val_ratio*100}% val, and {test_ratio*100}% test.\")\n",
    "\n",
    "# Run the split\n",
    "split_dataset(DATASET_DIR, OUTPUT_DIR, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39566348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # if needed again\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # RGB\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('dataset_split/train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('dataset_split/val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder('dataset_split/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b2d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PlantDiseaseCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PlantDiseaseCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)  # After 2 poolings, 224→112→56→28→14\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [B, 32, 112, 112]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [B, 64, 56, 56]\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # [B, 128, 28, 28]\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # [B, 256, 14, 14]\n",
    "\n",
    "        x = x.view(-1, 256 * 14 * 14)         # Flatten\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5e89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069f1544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())           # Should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4a8ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU device name: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0198a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/10 | Train Loss: 0.7675 | Train Acc: 73.96% | Val Loss: 0.3042 | Val Acc: 89.61% | 🕒 Time: 288.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2/10 | Train Loss: 0.2863 | Train Acc: 90.45% | Val Loss: 0.2107 | Val Acc: 92.81% | 🕒 Time: 279.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3/10 | Train Loss: 0.1806 | Train Acc: 93.79% | Val Loss: 0.1918 | Val Acc: 93.60% | 🕒 Time: 279.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4/10 | Train Loss: 0.1313 | Train Acc: 95.73% | Val Loss: 0.1934 | Val Acc: 93.88% | 🕒 Time: 284.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5/10 | Train Loss: 0.1110 | Train Acc: 96.26% | Val Loss: 0.1739 | Val Acc: 94.61% | 🕒 Time: 291.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6/10 | Train Loss: 0.0883 | Train Acc: 97.11% | Val Loss: 0.1861 | Val Acc: 94.74% | 🕒 Time: 282.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7/10 | Train Loss: 0.0798 | Train Acc: 97.50% | Val Loss: 0.1579 | Val Acc: 95.60% | 🕒 Time: 275.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8/10 | Train Loss: 0.0692 | Train Acc: 97.78% | Val Loss: 0.2127 | Val Acc: 94.30% | 🕒 Time: 264.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9/10 | Train Loss: 0.0690 | Train Acc: 97.91% | Val Loss: 0.1644 | Val Acc: 95.73% | 🕒 Time: 264.72s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10/10 | Train Loss: 0.0656 | Train Acc: 98.07% | Val Loss: 0.1797 | Val Acc: 95.04% | 🕒 Time: 307.18s\n",
      "💾 Model training complete and saved as 'plant_disease_cnn.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = PlantDiseaseCNN(num_classes=15).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ======== TRAINING =========\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"🚂 Epoch {epoch+1}/{num_epochs} - Training\", leave=False)\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)  # 🔧 accumulate weighted loss\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    train_loss = running_loss / total  # 🔧 average loss\n",
    "\n",
    "    # ======== VALIDATION =========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"🔍 Epoch {epoch+1}/{num_epochs} - Validating\", leave=False)\n",
    "        for images, labels in val_loader_tqdm:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)  # 🔧 accumulate weighted loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_loss = val_loss / val_total  # 🔧 average loss\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | \"\n",
    "          f\"🕒 Time: {duration:.2f}s\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), 'plant_disease_cnn.pth')\n",
    "print(\"💾 Model training complete and saved as 'plant_disease_cnn.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6be5a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Testing: 100%|██████████| 526/526 [00:42<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "\n",
      "                                             precision    recall  f1-score   support\n",
      "\n",
      "              Pepper__bell___Bacterial_spot       0.97      0.93      0.95      1313\n",
      "                     Pepper__bell___healthy       0.98      0.96      0.97      1902\n",
      "                      Potato___Early_blight       0.98      0.98      0.98       816\n",
      "                       Potato___Late_blight       0.92      0.97      0.94       720\n",
      "                           Potato___healthy       0.96      0.96      0.96       752\n",
      "                      Tomato_Bacterial_spot       0.97      0.95      0.96      1367\n",
      "                        Tomato_Early_blight       0.89      0.87      0.88       720\n",
      "                         Tomato_Late_blight       0.96      0.92      0.94      1243\n",
      "                           Tomato_Leaf_Mold       0.91      0.98      0.94      1257\n",
      "                  Tomato_Septoria_leaf_spot       0.93      0.95      0.94      1185\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite       0.94      0.95      0.94      1133\n",
      "                        Tomato__Target_Spot       0.90      0.95      0.92       965\n",
      "      Tomato__Tomato_YellowLeaf__Curl_Virus       0.99      0.96      0.98      1874\n",
      "                Tomato__Tomato_mosaic_virus       0.96      0.96      0.96       507\n",
      "                             Tomato_healthy       0.99      0.99      0.99      1073\n",
      "\n",
      "                                   accuracy                           0.95     16827\n",
      "                                  macro avg       0.95      0.95      0.95     16827\n",
      "                               weighted avg       0.95      0.95      0.95     16827\n",
      "\n",
      "\n",
      "✅ Test Accuracy: 95.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:59: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:60: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(\"results/confusion_matrix.png\")\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:71: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:72: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(\"results/per_class_accuracy.png\")\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:101: UserWarning: Glyph 10060 (\\N{CROSS MARK}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:102: UserWarning: Glyph 10060 (\\N{CROSS MARK}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(\"results/misclassified_examples.png\")\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:117: UserWarning: Glyph 128230 (\\N{PACKAGE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Muhammad Iqbal\\AppData\\Local\\Temp\\ipykernel_10168\\283138403.py:118: UserWarning: Glyph 128230 (\\N{PACKAGE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(\"results/class_distribution.png\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions saved to 'results/test_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ensure results directory exists\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ✅ Reload trained model\n",
    "model = PlantDiseaseCNN(num_classes=15).to(device)\n",
    "model.load_state_dict(torch.load('plant_disease_cnn.pth'))\n",
    "model.eval()\n",
    "\n",
    "# ✅ Predict on Test Data\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"🔍 Testing\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# ✅ 1. Classification Report\n",
    "print(\"\\n📊 Classification Report:\\n\")\n",
    "report = classification_report(all_labels, all_preds, target_names=test_dataset.classes)\n",
    "print(report)\n",
    "\n",
    "# Save report as text file\n",
    "with open(\"results/classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# ✅ 2. Overall Test Accuracy\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n✅ Test Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# ✅ 3. Confusion Matrix Heatmap\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=test_dataset.classes,\n",
    "            yticklabels=test_dataset.classes,\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            annot_kws={\"size\": 7})\n",
    "plt.title(\"📊 Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# ✅ 4. Per-Class Accuracy Bar Plot\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "sns.barplot(x=test_dataset.classes, y=class_accuracy)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"📈 Per-Class Accuracy\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/per_class_accuracy.png\")\n",
    "plt.close()\n",
    "\n",
    "# ✅ 5. Misclassified Images Viewer (and Save)\n",
    "def show_misclassified_images(model, loader, class_names, device, num_images=10):\n",
    "    model.eval()\n",
    "    misclassified = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for img, pred, label in zip(images, preds, labels):\n",
    "                if pred != label:\n",
    "                    misclassified.append((img.cpu(), pred.cpu(), label.cpu()))\n",
    "                if len(misclassified) >= num_images:\n",
    "                    break\n",
    "            if len(misclassified) >= num_images:\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, (img, pred, label) in enumerate(misclassified):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(img.permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        plt.title(f\"Pred: {class_names[pred]}\\nTrue: {class_names[label]}\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"❌ Misclassified Examples\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/misclassified_examples.png\")\n",
    "    plt.close()\n",
    "\n",
    "show_misclassified_images(model, test_loader, test_dataset.classes, device)\n",
    "\n",
    "# ✅ 6. Test Class Distribution\n",
    "test_labels = [label for _, label in test_dataset]\n",
    "label_counts = Counter(test_labels)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "sns.barplot(x=[test_dataset.classes[i] for i in label_counts.keys()],\n",
    "            y=list(label_counts.values()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"📦 Class Distribution in Test Dataset\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/class_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# ✅ 7. Save Predictions to CSV\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image_id = test_dataset.imgs[i][0].split('/')[-1]\n",
    "            predicted_class = test_dataset.classes[predicted[i]]\n",
    "            predicted_probability = probabilities[i][predicted[i]].item()\n",
    "            predictions.append([image_id, predicted_class, predicted_probability])\n",
    "\n",
    "df_predictions = pd.DataFrame(predictions, columns=['image_id', 'predicted_class', 'predicted_probability'])\n",
    "df_predictions.to_csv('results/test_predictions.csv', index=False)\n",
    "print(\"✅ Predictions saved to 'results/test_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab467b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
